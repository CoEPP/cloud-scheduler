#!/usr/bin/env python
# vim: set expandtab ts=4 sw=4:

# Copyright (C) 2009 University of Victoria
# You may distribute under the terms of either the GNU General Public
# License or the Apache v2 License, as specified in the README file.

## Auth: Duncan Penfold-Brown. 6/15/2009

## CLOUD SCHEDULER
##
## The main body for the cloud scheduler, that encapsulates and organizes
## all cloud scheduler functionality.
##
## Using optparse for command line options (http://docs.python.org/library/optparse.html)
##


## Imports

import os
import sys
import time
import string
import getopt
import signal
import logging
import threading
import ConfigParser
import logging.handlers
from optparse import OptionParser

import cloudscheduler.config as config
import cloudscheduler.utilities as utilities
import cloudscheduler.__version__ as version
import cloudscheduler.info_server as info_server
import cloudscheduler.cloud_management as cloud_management
import cloudscheduler.job_management as job_management


## GLOBAL VARIABLES

usage_str = "cloud_scheduler [-f FILE | --config-file] [-c FILE | --cloud-config FILE] [-m SERVER | --MDS SERVER]"
version_str = "Cloud Scheduler " + version.version

log = logging.getLogger("cloudscheduler")
null_handler = utilities.NullHandler()
log.addHandler(null_handler)


## Thread Bodies

# Polling:
# The resource & running vm polling thread. Inherits from Thread class.
# Polling thread will iterate through the resource pool, updating resource
#   status based on vm_poll calls to each vm in a cluster's 'vms' list (of
#   running vms)
# Constructed with argument 'resource_pool'
class PollingTh(threading.Thread):

    def __init__(self, resource_pool):
        threading.Thread.__init__(self)
        self.resource_pool = resource_pool

    def run(self):
        # TODO: Implement polling thread functionality here.
        log.info("Starting polling thread...")

# Scheduling:
# Scheduling thread will match jobs to available resources and start vms
# (for now, will contain test create/destroy functions)
class SchedulingTh(threading.Thread):

    def __init__(self, resource_pool, job_pool):
        threading.Thread.__init__(self)
        self.resource_pool = resource_pool
        self.job_pool      = job_pool
        self.quit          = False
        self.scheduling_interval = 5 # seconds
        self.starting_poll_interval = 120 # 2 minutes
        self.running_poll_interval = 900 # 15 minutes


    def stop(self):
        log.debug("Waiting for scheduling loop to end")
        self.quit = True

    def run(self):
        log.info("Starting scheduling thread...")
        prevMachineList = []
        ########################################################################
        ## Full scheduler loop
        ########################################################################

        while (not self.quit):
            log_with_line("Scheduler Cycle:")

            ## Query the job pool to get new unscheduled jobs
            # Populates the 'jobs' and 'scheduled_jobs' lists appropriately
            log_with_line("Querying Condor job pool (via SOAP)")
            condor_jobs = self.job_pool.job_querySOAP()
            if (condor_jobs == None):
                log.error("Failed to contact Condor job scheduler. Continuing with VM management.")
            else:
                self.job_pool.update_jobs(condor_jobs)

            ## Check that jobs are valid for the clusters available
            bad_jobs = []
            for user in self.job_pool.new_jobs.keys():
                for job in self.job_pool.new_jobs[user]:
                    if not self.resource_pool.resourcePF(job.req_network, job.req_cpuarch):
                        bad_jobs.append(job)
            while len(bad_jobs) > 0:
                self.job_pool.remove_system_job(bad_jobs.pop())

            ## Figure out distribution of VMs requested and available
            desired_types = self.job_pool.job_type_distribution()
            current_types = self.resource_pool.vmtype_distribution()
            # Negative difference means will need to create that type
            diff_types = {}
            for type in current_types.keys():
                if type in desired_types.keys():
                    diff_types[type] = current_types[type] - desired_types[type]
                else:
                    diff_types[type] = 0
            for type in desired_types.keys():
                if type not in current_types.keys():
                    diff_types[type] = -desired_types[type]

            ## Schedule user jobs
            # TODO: This should be less explicit in its reference to JobPool class parts.
            #       Something like 'for job in self.job_pool.get_unsched_jobs()'
            log.debug("Attempt to schedule user jobs")
            for user in self.job_pool.new_jobs.keys():

                # Attempt to schedule jobs in order of their appearance in user's job list
                # (currently sorted by Job priority)
                for job in self.job_pool.new_jobs[user]:
                    # Check that type of VM for job is needed
                    if job.req_vmtype in diff_types.keys() and diff_types[job.req_vmtype] <= 0:
                        # Find resources that match the job's requirements
                        (pri_rsrc, sec_rsrc) = self.resource_pool.get_resourceBF(job.req_network, \
                        job.req_cpuarch, job.req_memory, job.req_cpucores, job.req_storage)
                        
                        good_resources = [pri_rsrc]
                        if sec_rsrc and pri_rsrc != sec_rsrc:
                            good_resources.append(sec_rsrc)

                        # If no resource fits, continue to next job in user's list
                        if good_resources[0] == None:
                            log.info("No resource to match job: %s" % job.id)
                            log.info("Leaving job unscheduled, moving to %s's next job" % user)
                            continue


                        # Create an optional customization metadata file
                        customizations = []
                        if config.condor_host != "localhost" and config.condor_context_file:
                            customizations.append((config.condor_host, config.condor_context_file))

                        if config.cert_file:
                            file_contents = open(config.cert_file).read()

                            if config.cert_file_on_vm:
                                file_location = config.cert_file_on_vm
                            else:
                                file_location = config.cert_file

                            customizations.append((file_contents, file_location))

                        if config.key_file:
                            file_contents = open(config.key_file).read()

                            if config.key_file_on_vm:
                                file_location = config.key_file_on_vm
                            else:
                                file_location = config.key_file

                            customizations.append((file_contents, file_location))


                        for resource in good_resources:
                            # Print details of the resource selected
                            log.debug("Booting VM for job %s on %s:" % (job.id, resource.name))
                            resource.log()

                            if resource.__class__.__name__ == "NimbusCluster":
                                create_ret = pri_rsrc.vm_create(job.req_image,
                                    job.req_vmtype, job.req_network, job.req_cpuarch,
                                    job.req_imageloc, job.req_memory, job.req_cpucores,
                                    job.req_storage, customization=customizations)
                            elif resource.__class__.__name__ == "EC2Cluster":
                                create_ret = pri_rsrc.vm_create(job.req_image,
                                    job.req_vmtype, job.req_network, job.req_cpuarch,
                                    job.req_ami, job.req_memory, job.req_cpucores,
                                    job.req_storage, customization=customizations)

                            # If the VM create fails, try again on another resource
                            if (create_ret != 0):
                                log.debug("Creating VM for job %s failed on %s. " % (job.id, resource.name))
                                continue

                        # If VM creation fails for user-job on all resources move to next user
                        if create_ret != 0:
                            log.debug("None of the resources could boot a vm for job %s. " % job.id + \
                                      "Leaving %s's jobs unscheduled, moving on to next user" % user)
                            break

                        # Mark job as scheduled
                        self.job_pool.schedule(job)
                        # break out of this user's job and try next user
                        break
                    else:
                        # This user already has their share
                        break
                # ENDFOR - for jobs in user's unscheduled job set
            #ENDFOR - Attempt to schedule each one job per user


            ## Clear all un-needed VMs from the system
            log.debug("Clearing all un-needed VMs from the system")

            # Count the number of jobs that require a certain VM type,
            # and then destroy the EXCESS VMs in that type TODO: leave a few for spare?
            log.debug("Gathering required VM types.")
            required_vmtypes = self.job_pool.get_required_vmtypes()

            machineList = self.resource_pool.resource_querySOAP()
            if machineList:
                available_vmtypes_dict = self.resource_pool.get_vmtypes_count(machineList)
                required_vmtypes_dict = self.job_pool.get_required_vmtypes_dict()
                # Remove excess VMs when available VM type exceedes required by jobs
                log.debug("Removing excess VMs from the system.")
                to_remove = {}
                for type, count in available_vmtypes_dict.iteritems():
                    if type in required_vmtypes_dict.keys():
                        if count > required_vmtypes_dict[type]:
                            to_remove[type] = count - required_vmtypes_dict[type]
                    # This type no longer needed, remove all remaining
                    else:
                        to_remove[type] = count
                # Go over the types and find idle machines to remove
                for type, count in to_remove.iteritems():
                    log.debug("Attemping to remove %i VMs of type %s" % (count, type))
                    criteria = {'VMType': type, 'State': 'Unclaimed', 'Activity': 'Idle'}
                    unused_vms_of_type = self.resource_pool.find_in_where(machineList, criteria)
                    num_to_shutdown = 0
                    len_unused = len(unused_vms_of_type)
                    # Make sure we don't try to shutdown more than is possible
                    if len_unused == 0:
                        log.debug("Could not find any VMs to shutdown")
                    elif len_unused >= count:
                        num_to_shutdown = count
                    elif len_unused < count:
                        num_to_shutdown = len_unused
                    for x in range(0, num_to_shutdown):
                        log.debug("Name of Condor Machine to shutdown: %s" % unused_vms_of_type[x]['Name'])
                        condor_names = unused_vms_of_type[x]['Name'].split('.')
                        condor_name = condor_names[0]
                        # Track if machine found or not so can break from loop early
                        found_vm = False
                        for cluster in self.resource_pool.resources:
                            for vm in cluster.vms:
                                log.debug("vm hostname: %s, condor name: %s" % (vm.hostname, condor_name))
                                if vm.hostname.split(".")[0] == condor_name:
                                    found_vm = True
                                    log.info("VM of type %s no longer required for remaining jobs" % vm.vmtype)
                                    vm.log_dbg()
                                    destroy_ret = cluster.vm_destroy(vm)
                                    if destroy_ret != 0:
                                        log.error("Destroying VM failed in attempt to clear uneeded VM. Leaving VM.")
                                    else:
                                        break
                            if found_vm:
                                break
                # Compare current Machine List with Previous
                # Looking for machines that have changed jobs since last check
                # and are of a type that has an higher than wanted distribution
                # based on diff_types
                vm_matches = []
                #vm_starting = []
                changed = self.resource_pool.machine_jobs_changed(machineList, prevMachineList)
                for cluster in self.resource_pool.resources:
                    for vm in cluster.vms:
                        if vm.hostname in changed:
                            vm_matches.append(vm)
                        #if vm.status == "Starting":
                        #    vm_starting.append(vm)
                num_to_change = {}
                vm_count = self.resource_pool.vm_count()
                for type, val in diff_types.iteritems():
                    num_to_change[type] = int(round(val * vm_count))
                to_shutdown = []
                #for vm in vm_starting:
                #    if vm.vmtype in num_to_change.keys() and num_to_change[vm.vmtype] > 0:
                #        to_shutdown.append(vm)
                #        num_to_change[vm.vmtype] -= 1
                for vm in vm_matches:
                    if vm.vmtype in num_to_change.keys() and num_to_change[vm.vmtype] > 0:
                        to_shutdown.append(vm)
                        num_to_change[vm.vmtype] -= 1
                for vm in to_shutdown:
                    log.debug("Shutting down VM of type %s to rebalance resources" % vm.vmtype)
                    cluster = self.resource_pool.get_cluster_with_vm(vm)
                    log.debug("VM %s on cluster %s shutting down" % (vm.name, cluster.name))
                    destroy_ret = cluster.vm_destroy(vm)
                    if destroy_ret != 0:
                        log.error("Destroying VM failed in attempt to redistribute VMs. Leaving VM.")

                prevMachineList = machineList
            else:
                log.debug("No Machines returned by Condor Collector Query")
            ## Poll all remaining system VMs
            log.debug("Polling all running VMs...")

            for cluster in self.resource_pool.resources:
                for vm in cluster.vms:
                    now = int(time.time())

                    if vm.lastpoll and (vm.status == "Starting" and now - vm.lastpoll < self.starting_poll_interval):
                        log.debug("Skip polling %s, which has status %s" % (vm.id, vm.status))
                        continue
                    elif vm.lastpoll and (vm.status == "Running" and now - vm.lastpoll < self.running_poll_interval):
                        log.debug("Skip polling %s, which has status %s" % (vm.id, vm.status))
                        continue

                    ret_state = cluster.vm_poll(vm)

                    # Print polled VM's state and details
                    log.debug("Polled VM: ")
                    vm.log_dbg()

                    ## If the VM is in an error state, keep track of error and
                    # after passing some threshold destroy the machine.
                    # Decrementing to avoid a long running VM from accumulating periodic
                    # errors and being destroyed
                    if ret_state == "Error":
                        vm.errorcount += 1
                    elif vm.errorcount > 0:
                        vm.errorcount -= 1
                    # Implement a config option to set this?
                    if vm.errorcount >= config.polling_error_threshold:
                        log.info("VM %s in error state. Destroying..." % vm.name)

                        # Destroy the VM
                        destroy_ret = cluster.vm_destroy(vm)
                        if (destroy_ret != 0):
                            log.error("Destroying VM failed. Leaving VM in error state.")
                            continue

                    # ENDIF - if VM in error state
                # ENDFOR - for each VM in cluster vms list

            ## Wait for a number of seconds
            log.debug("Scheduler - Waiting %ss" % self.scheduling_interval)
            time.sleep(self.scheduling_interval)

            # ENDFOR - For each cluster in the resource pool
        # ENDWHILE - End of the main scheduler loop

        # Exit the scheduling thread - clean up VMs and exit
        log.info("Exiting scheduler thread")

        # Destroy all VMs and finish
        remaining_vms = []
        log_with_line("Destroying all remaining VMs and exiting :-(")
        for cluster in self.resource_pool.resources:
            i = len(cluster.vms)
            while (i != 0):
                i = i-1

                log.debug("Destroying VM:")
                cluster.vms[i].log
                destroy_ret = cluster.vm_destroy(cluster.vms[i])
                if destroy_ret != 0:
                    log.error("Destroying VM failed. Continuing anyway... check VM logs")
                    remaining_vms.append(cluster.vms[i])
        #ENDFOR - Attempt to destroy each remaining VM in the system

        # Print list of VMs cloud scheduler failed to destroy before exit.
        if (remaining_vms != []):
            log.error("The following VMs could not be destroyed properly:")
            for vm in remaining_vms:
                log.error("VM: %s, ID: %s" % (vm.name, vm.id))


##
## Functions
##

def main():
    # Log entry message (for timestamp in log)
    log.info("Cloud Scheduler system starting...")

    # Create a parser and process commandline arguments
    parser = OptionParser(usage=usage_str, version=version_str)
    set_options(parser)
    (cli_options, args) = parser.parse_args()

    # Look for global configuration file, and initialize config
    if (cli_options.config_file):
        config.setup(path=cli_options.config_file)
    else:
        config.setup()

    # Set up logging
    log.setLevel(utilities.LEVELS[config.log_level])
    # TODO: Requires Python 2.5+
    #log_formatter = logging.Formatter("%(asctime)s - %(levelname)s " \
    #                                  "- %(funcName)s: %(message)s")
    log_formatter = logging.Formatter("%(asctime)s - %(levelname)s " \
                                      "- %(lineno)4d - %(message)s")
    if config.log_stdout:
        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(log_formatter)
        log.addHandler(stream_handler)

    if config.log_location:
        file_handler = None
        if config.log_max_size:
            file_handler = logging.handlers.RotatingFileHandler(
                                            config.log_location,
                                            maxBytes=config.log_max_size)
        else:
            file_handler = logging.handlers.RotatingFileHandler(
                                            config.log_location,)

        file_handler.setFormatter(log_formatter)
        log.addHandler(file_handler)

    if not config.log_location and not config.log_stdout:
        null_handler = utilities.NullHandler()
        log.addHandler(null_handler)


    # Command line options take precedence, so replace config file
    # option with command line option
    if cli_options.cloud_conffile:
        config.cloud_resource_config = cli_options.cloud_conffile

    # If the neither the cloud conffile or the MDS server are passed to obtain
    # initial cluster information, print usage and exit the system.
    if (not config.cloud_resource_config) and (not cli_options.mds_server):
        print "ERROR - main - No cloud or cluster information sources provided"
        parser.print_help()
        sys.exit(1)

    # Create a job pool
    job_pool = job_management.JobPool("Job Pool")

    # Create a resource pool
    cloud_resources = cloud_management.ResourcePool("Resource Pool")
    cloud_resources.setup(config.cloud_resource_config)

    # TODO: Add code to query an MDS to get initial cluster/cloud information

    # Log the resource pool
    cloud_resources.log_pool()

    # TODO: Resolve issue of atomicity / reliability when 2 threads are working
    #       on the same resource pool data. Does it matter (best effort!)?

    # Start the cloud scheduler info server for RPCs
    info_serv = info_server.CloudSchedulerInfoServer(cloud_resources)
    info_serv.daemon = True
    info_serv.start()

    # Create the Polling thread (pass resource pool)
    poller = PollingTh(cloud_resources)
    poller.start()

    # Create the Scheduling thread (pass resource pool)
    scheduler = SchedulingTh(cloud_resources, job_pool)
    scheduler.start()

    # Set SIGTERM (kill) handler
    signal.signal(signal.SIGTERM, term_handler)

    # Wait for keyboard input to exit the cloud scheduler
    try:
        while scheduler.isAlive():
            time.sleep(2)
    except (SystemExit, KeyboardInterrupt):
        log.info("Exiting normally due to KeyboardInterrupt or SystemExit")

    # Clean up out threads (shuts down all running VMs)
    log.info("System exiting gracefully")
    scheduler.stop()
    scheduler.join()
    info_serv.stop()
    info_serv.join()
    sys.exit()

def term_handler(signal, handler):
    log.info("Recieved SIGTERM signal")
    sys.exit()

# Sets the command-line options for a passed in OptionParser object (via optparse)
def set_options(parser):

    # Option attributes: action, type, dest, help. See optparse documentation.
    # Defaults: action=store, type=string, dest=[name of the option] help=none
    parser.add_option("-f", "--config-file", dest="config_file",
                      metavar="FILE",
                      help="Designate a config file for Cloud Scheduler")
    parser.add_option("-c", "--cloud-config", dest="cloud_conffile",
                      metavar="FILE",
                      help="Designate a config file from which cloud cluster "
                           "information is obtained")

    parser.add_option("-m", "--MDS", dest="mds_server", metavar="SERVER",
                      help="Designate an MDS server from which cloud cluster "
                           "information is obtained")


# logs readable lined lines across the screen with message
def log_with_line(msg):
    msg_len = len(msg)
    fill = "-" * (120-msg_len)
    log.debug("-----"+msg+fill)


##
## Main Functionality
##

main()
